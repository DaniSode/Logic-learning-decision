{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b1a0d62713e333da3863139a792884a",
     "grade": false,
     "grade_id": "cell-6ec5aa4507a8f918",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "# Logic, Learning, and Decision\n",
    "\n",
    "## Home Assignment 3\n",
    "\n",
    "### Model Checking with $\\mu%$-Calculus & Controlling through Q-Learning\n",
    "</center>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11adea9af6cd1ee5506870530da8786d",
     "grade": false,
     "grade_id": "cell-d299ce342e54a4d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The first objective of this home assignment is to deepen the understanding of temporal logic specifications and model checking algorithms based on $\\mu$-calculus. You will achieve that through implementing a fixed-point algorithm for a particular _CTL*_ specification and test it out on a booking problem of variable size. \n",
    "\n",
    "The second objective is to obtain a basic understanding of a central Reinforcement Learning algorithm called *Q-learning*.\n",
    "\n",
    "This home assignment is performed in *two member groups*. Write all your answers into this notebook and **submit only this notebook (.ipynb) on Canvas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac0abd938ff82f6c25852940e9da18c3",
     "grade": false,
     "grade_id": "cell-8dfe6514b5b0e564",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Comments and Recommendations\n",
    "As always:\n",
    "* The following resources will be of great help to you for this assignment:\n",
    "  * Lecture Notes\n",
    "  * [Python docs](https://docs.python.org/3/)\n",
    "  * [Google](https://www.google.com)\n",
    "* This assignment is written for Python 3.5 or later!\n",
    "* We will test your code with additional edge cases. So convince yourself that everything is correct before you submit.\n",
    "* This assignment makes use of the Python packages [numpy](https://docs.scipy.org/doc/numpy/) and [matplotlib](https://matplotlib.org/index.html). Make sure to have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc04b29c5d48f53f509114009f833954",
     "grade": false,
     "grade_id": "cell-d0ce19473bf6c277",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    print(\"You need to install numpy! Open a command prompt and run 'pip install numpy'\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"You need to install matplotlib! Open a command prompt and run 'pip install matplotlib'\")\n",
    "\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    try:\n",
    "        import msvc_runtime\n",
    "    except ImportError:\n",
    "        print(\"You may need to install msvc-runtime for multiprocessing! \" + \n",
    "        \"If you have issues with the function `eval_hyperparam_grid`, open \" + \n",
    "        \"a command prompt and run 'pip install msvc-runtime'. Then restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d11fe46a5b8dc42a36e20f63dabc85b7",
     "grade": false,
     "grade_id": "cell-c55d42dab3dd9445",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65d5efc21901317a346d6893f17cc7ca",
     "grade": false,
     "grade_id": "cell-950323411a5504b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Model Checking with $\\mu-$Calculus\n",
    "\n",
    "Imagine you are employed by a company, but you are quite unhappy recently, because the system you are working with (for instance, a manufacturing cell, but could be anything else) exhibits some strange behavior and just freezes randomly. So, you decide to model the system (i.e. manufacturing cell, etc) and analyze it with the methods from your favorite course at university. Hence, you come up with the petri net $P_1$:\n",
    "![petri net](fig/petri_net.png)\n",
    "Two parallel processes require two resources $R_1$ and $R_2$ for their operations. This is a classic booking problem. Although appearing simple, this system may exhibit undesirable behavior. For the majority of Part 1 of the assignment we will work with this system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a220242a51351383ac07ca4ffe618065",
     "grade": false,
     "grade_id": "cell-cf7667e2af990fc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import PetriNet\n",
    "from util import Place, Arc\n",
    "from util import plot_petrinet\n",
    "\n",
    "\n",
    "def make_petrinet(p11_tokens, p21_tokens, R1_tokens, R2_tokens):\n",
    "    return PetriNet(places=[Place('p11', p11_tokens), Place('p12', 0),\n",
    "                            Place('p21', p21_tokens), Place('p22', 0),\n",
    "                            Place('R1', R1_tokens), Place('R2', R2_tokens)],\n",
    "                    transitions={'a1', 'b1', 'a2', 'b2'},\n",
    "                    arcs={Arc('p11', 'a1', 1),\n",
    "                          Arc('a1', 'p12', 1),\n",
    "                          Arc('p12', 'b1', 1),\n",
    "                          Arc('b1', 'p11', 1),\n",
    "                          Arc('R1', 'a1', 1),\n",
    "                          Arc('R1', 'b2', 1),\n",
    "                          Arc('b1', 'R1', 1),\n",
    "                          Arc('b2', 'R1', 1),\n",
    "                          Arc('R2', 'a2', 1),\n",
    "                          Arc('R2', 'b1', 1),\n",
    "                          Arc('b1', 'R2', 1),\n",
    "                          Arc('b2', 'R2', 1),\n",
    "                          Arc('p21', 'a2', 1),\n",
    "                          Arc('a2', 'p22', 1),\n",
    "                          Arc('p22', 'b2', 1),\n",
    "                          Arc('b2', 'p21', 1)})\n",
    "\n",
    "\n",
    "P_1 = make_petrinet(p11_tokens=3, p21_tokens=2, R1_tokens=2, R2_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b44b5200f463855f586716e75ca24f5c",
     "grade": false,
     "grade_id": "cell-2e491eff08188b4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have also implemented a function for you in the PetriNet class that generates the corresponding reachability graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5460c1ef947f9e4a89fa20a40c1471f",
     "grade": false,
     "grade_id": "cell-bfdddd0d867fad17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import plot_digraph\n",
    "\n",
    "print('P_1:')\n",
    "plot_digraph(P_1.make_reachability_graph(), 'fig/P_1_reach_graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "970cd86c0c8c32f64c53ca29beb166c1",
     "grade": false,
     "grade_id": "cell-f89f8e548997b4a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To be able to use $\\mu$-calculus to check the Petri net model, we need to transform it into a transition system, though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed9f9ab2054716282c9a2595bf43d707",
     "grade": false,
     "grade_id": "cell-9b3b3c7c1016cf98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a48f4a8a210ad9fbbb47d022b22424bd",
     "grade": false,
     "grade_id": "cell-3dfb2dc445d04d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Transition Systems\n",
    "\n",
    "In this assignment, we will work with a more general model for discrete event systems called __Transition System__. A transition system $G$ is defined by a 6-tuple $G = \\langle X, \\Sigma, T, I, AP, \\lambda \\rangle$ where $X$ is a set of states, $\\Sigma$ is a finite set of events, $T \\subseteq X \\times T \\times X$ is a transition relation, where a transition $t = (x, a, x') \\in T$, includes the source state $x$, the event label $a$, and the target state $x'$, $I \\subseteq X$ is a set of possible initial states, $AP$ is a set of atomic propositions, and $\\lambda: X \\mapsto 2^{AP}$ is a state labeling function. A transition system  $G$ without the state labels, where $AP$ and $\\lambda$ are excluded from $G$ is obviously an automaton without marked and forbidden states.\n",
    "\n",
    "In order to implement a data structure corresponding to a transition system, we introduce a new class of _State_ objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4905c5e7328664c5fcc02b90c3449ce7",
     "grade": false,
     "grade_id": "cell-380888be34de0178",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    \n",
    "    def __init__(self, name, labels=None):\n",
    "        \"\"\"\n",
    "        Constructor  of the state.\n",
    "        \n",
    "        :param name: String. Default atomic proposition of the state\n",
    "        :param labels: Set of atomic propositions that a true in the state\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        assert labels is None or type(labels) is set\n",
    "        self.labels = {name} if not labels else labels | {name}\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Prints the state in a pretty way.\"\"\"\n",
    "        return 'name: {} & ' \\\n",
    "               'labels: {}'.format(self.name, self.labels)\n",
    "        \n",
    "    def is_satisfied(self, atomic_proposition):\n",
    "        \"\"\"Checks whether the atomic proposition is statisfied in the state.\"\"\"\n",
    "        return atomic_proposition in self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5c55fdaa6db01b133b10b6778f0d3db",
     "grade": false,
     "grade_id": "cell-9d2b45a323388413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That allows us to define the _TransitionSystem_ class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ffa219bd6b989a023788e222eeacd93",
     "grade": false,
     "grade_id": "cell-244d255993c355a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import Transition\n",
    "\n",
    "\n",
    "class TransitionSystem(object):\n",
    "\n",
    "    def __init__(self, states, init, events, trans):\n",
    "        \"\"\"\n",
    "        This is the constructor of the transition system.\n",
    "\n",
    "        At creation, the automaton gets the following attributes assigned:\n",
    "        :param states: A set of States\n",
    "        :param init: A set of initial States\n",
    "        :param events: A set of events\n",
    "        :param trans: A set of transitions\n",
    "        \"\"\"\n",
    "        assert isinstance(states, set)\n",
    "        self.states = states\n",
    "        assert isinstance(init, set)\n",
    "        self.init = init\n",
    "        assert isinstance(events, set)\n",
    "        self.events = events\n",
    "        assert isinstance(trans, set)\n",
    "        self.trans = trans\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Prints the transition system in a pretty way.\"\"\"\n",
    "        states_str = '{\\n\\t' + ',\\n\\t'.join(\n",
    "            [str(s) for s in self.states]) + '\\n\\t}'\n",
    "        init_str = '{\\n\\t' + ', '.join([str(s.name) for s in self.init]) + '\\n\\t}'\n",
    "        trans_str = '\\n\\t'.join(\n",
    "            ['{} --{}--> {},'.format(t.source.name, t.event, t.target.name) for t in self.trans])\n",
    "        trans_str = '{\\n\\t' + trans_str + '\\n\\t}'\n",
    "        return 'states: \\n\\t{}\\n' \\\n",
    "               'init: \\n\\t{}\\n' \\\n",
    "               'events: \\n\\t{}\\n' \\\n",
    "               'transitions: \\n\\t{}\\n'.format(\n",
    "                   states_str, init_str, self.events, trans_str)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Checks if two transition systems are the same\"\"\"\n",
    "        if type(other) is type(self):\n",
    "            return self.__dict__ == other.__dict__\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0feffb62f1271e6c008f8daab84e22ef",
     "grade": false,
     "grade_id": "cell-74ff7f77c329d6a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we can translate our Petri net $P_1$ of the booking problem that we are working with into a *TransitionSystem* via its reachability graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a67aac459273877ae2da708e35577b",
     "grade": false,
     "grade_id": "cell-61168b606b0b8afd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import plot_transitionsystem\n",
    "\n",
    "\n",
    "def make_transition_system(petri_net):\n",
    "    \"\"\"Transforms a PetriNet to a TransitionSystem.\"\"\"\n",
    "    G = petri_net.make_reachability_graph()\n",
    "    states = {State(n) for n in G.nodes}\n",
    "    \n",
    "    def get_state(node):\n",
    "        for s in states:\n",
    "            if s.name == node:\n",
    "                return s\n",
    "    \n",
    "    init = {get_state(G.init)}\n",
    "    events = petri_net.transitions\n",
    "    trans = {Transition(get_state(e.source), e.label, get_state(e.target)) for e in G.edges}\n",
    "    return TransitionSystem(states, init, events, trans)\n",
    "\n",
    "\n",
    "T_1 = make_transition_system(P_1)\n",
    "print('T_1:')\n",
    "plot_transitionsystem(T_1, 'fig/P_1_transition_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3368b206e875792bff56831d018ef7ed",
     "grade": false,
     "grade_id": "cell-64e841c2f47c6614",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5e6cdc2d39a43dee1d9f4c79a46a2df",
     "grade": false,
     "grade_id": "cell-a9f3f4ebf3699dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.1 \n",
    "\n",
    "### ___CTL*___-Specification to $\\mu$-Calculus *[2p]*\n",
    "\n",
    "In this assignment, you will implement the $\\mu$-calculus algorithms for the booking problem $P_i$ shown above. Specifically, you will check its TransitionSystem model $T_i$ for the following temporal logic specification given in ___CTL*___:\n",
    "\n",
    "<center>\n",
    "    $\\varphi_i = \\forall \\square \\exists \\lozenge m_i$,\n",
    "</center>\n",
    "with $m_i$ indicating a specific initial marking of $P_i$ (e.g. $m_1 = [3 0 2 0 2 1]$). \n",
    "\n",
    "* In simple words, what does this specification $\\varphi_1 = \\forall \\square \\exists \\lozenge m_1$ say and why does it specify a desired behavior? Write your answer below. ***[1p]***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e1e7a9738011d7117bc4079b1645aaf",
     "grade": true,
     "grade_id": "spec_in_words",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dc13cf29f0ef2f951efe970bcafa750",
     "grade": false,
     "grade_id": "cell-a1922ae5d4d411ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, \n",
    "* transform the ___CTL*___ formula $\\varphi_1$ to $\\mu$-calculus set expressions. Show also intermediate results. ***[1p]***\n",
    "* *Note*: [This webpage](http://detexify.kirelabs.org/classify.html) may be useful for finding the right LaTex symbols.\n",
    "* *Hint:* The double square brackets $[\\![p]\\!]$ can be done with [ \\ ! [ p ] \\ ! ] within the math environment. The LaTeX environments of the [amsmath package](https://tex.stackexchange.com/questions/3782/how-can-i-split-an-equation-over-two-or-more-lines) are also supported (equation, multiline and split)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336f5fbe458b8980c23c131a33b3996a",
     "grade": true,
     "grade_id": "cell-97e5d76cea7a732d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c6e305cc0c1027046d34d4512df2d4d",
     "grade": false,
     "grade_id": "cell-2b90678f9da35277",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "277cfaaa94ce71952ff2d1966b4586b3",
     "grade": false,
     "grade_id": "cell-eba6f74362201fa1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.2 \n",
    "### $\\text{Pre}^\\exists$ Operator *[1p]*\n",
    "\n",
    "$\\mu$-calculus includes next modality functions $f \\in \\mathcal{F}$, namely $f = \\exists \\bigcirc$ and $f = \\forall \\bigcirc$. You will implement $f = \\exists \\bigcirc$ as __predecessor set operation__ in this section of the assignment. We will need this operator to check our model of the booking problem. \n",
    "\n",
    "This set operator is defined as\n",
    "\n",
    "<center>\n",
    "   $\\text{Pre}^\\exists(Y) = \\{x \\mid (\\exists a \\in \\Sigma(x))\\delta(x, a) \\subseteq Y \\}$, \n",
    "</center>\n",
    "where $x \\in X$ and $Y \\in 2^X$.\n",
    "\n",
    "Now,\n",
    "* implement the $\\text{Pre}^\\exists$ set operator in the `pre_exists` function.\n",
    "* _Hint:_ The inbuilt Python function [`any`](https://docs.python.org/3/library/functions.html#any) might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb54d8925972a1800195efaa8b95f277",
     "grade": false,
     "grade_id": "cell-f35fd41037c8f6f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You might want to use one of these...\n",
    "from util import filter_trans_by_source, filter_trans_by_events, filter_trans_by_target\n",
    "from util import extract_elems_from_trans, flip_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "375c18b212193399f738ceeda6780e56",
     "grade": false,
     "grade_id": "pre_exists",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pre_exists(Y, ts):\n",
    "    \"\"\"\n",
    "    Returns the new set of states for which the exists next modality is true.\n",
    "    \n",
    "    :param Y: Set of States\n",
    "    :param ts: TransitionSystem\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your own tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6301576348c56abda380a784853cde6d",
     "grade": true,
     "grade_id": "pre_exists_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = State(1)\n",
    "s2 = State(2)\n",
    "\n",
    "T0 = TransitionSystem({s1}, {s1}, set(), set())\n",
    "assert pre_exists(set(), T0) == set()\n",
    "assert pre_exists({s1}, T0) == set()\n",
    "\n",
    "T1 = TransitionSystem({s1}, {s1}, {'a'}, {Transition(s1, 'a', s1)})\n",
    "assert pre_exists(set(), T1) == set()\n",
    "assert pre_exists({s1}, T1) == {s1}\n",
    "\n",
    "T2 = TransitionSystem({s1, s2}, {s1}, {'a'}, {Transition(s1, 'a', s2)})\n",
    "assert pre_exists(set(), T2) == set()\n",
    "assert pre_exists({s1}, T2) == set()\n",
    "assert pre_exists({s2}, T2) == {s1}\n",
    "assert pre_exists({s1, s2}, T2) == {s1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7029d24fbcf22b1c3183c8f882e54d7c",
     "grade": false,
     "grade_id": "cell-667a34de277b8c78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This $\\text{Pre}^\\forall$ set operator for $f = \\forall \\bigcirc$ is defined as\n",
    "\n",
    "<center>\n",
    "   $\\text{Pre}^\\forall(Y) = \\{x \\mid (\\forall a \\in \\Sigma(x))\\delta(x, a) \\subseteq Y \\}$, \n",
    "</center>\n",
    "where $x \\in X$ and $Y \\in 2^X$.\n",
    "\n",
    "An implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8214459668726adce417dd5a7d0ac938",
     "grade": false,
     "grade_id": "cell-32bdbb3bef5deab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pre_forall(Y, ts):\n",
    "    \"\"\"\n",
    "    Returns the new set of states for which the forall next modality is true.\n",
    "    \n",
    "    :param Y: Set of States\n",
    "    :param ts: TransitionSystem\n",
    "    \"\"\"\n",
    "    def all_t_into_Y_from(source):\n",
    "        out_transitions = filter_trans_by_source(ts.trans, {source})\n",
    "        return all({t.target in Y for t in out_transitions})\n",
    "    \n",
    "    Y = {x for x in ts.states if all_t_into_Y_from(x)}\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b30e2e9fe3c06d3c0d0f87f53a34ac3",
     "grade": false,
     "grade_id": "cell-2e046444f492bb00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "517be1e2f510664109649fcb081ae93e",
     "grade": false,
     "grade_id": "cell-963815443f9f10c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.3\n",
    "\n",
    "### Model Checking through Fixed-Point Iteration *[2p]*\n",
    "\n",
    "After having reformulated $\\varphi_i = \\forall \\square \\exists \\lozenge m_i$ as $\\mu$-calculus set expressions, and having obtained an implementation of both $\\text{Pre}^\\exists$ and $\\text{Pre}^\\forall$, we can start on the actual model checking algorithm. \n",
    "\n",
    "* Implement the function `is_always_eventually_satisfied` that takes as inputs an atomic proposition (e.g. $m_i$) and a TransitionSystem. It then checks whether the TransitionSystem satisfies $\\varphi_i$. ***[2p]***\n",
    "* _Hint:_ You need to implement both a least fixed-point iteration $\\mu Z$ and a greatest fixed-point iteration $\\nu Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4800761135b89b61bd058c4aa4544eb9",
     "grade": false,
     "grade_id": "check",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def is_always_eventually_satisfied(m_i, ts):\n",
    "    \"\"\"\n",
    "    Checks if a TransitionSystem always eventually satisfies an atomic proposition.\n",
    "    \n",
    "    :param m_i: String/integer. Atomic proposition\n",
    "    :param ts: TransitionSystem to check\n",
    "    \"\"\"\n",
    "    satisfied = False\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return satisfied\n",
    "\n",
    "\n",
    "m_1 = '[3 0 2 0 2 1]'\n",
    "print('T_1 {} satisfy phi_1!'.format('does' if is_always_eventually_satisfied(m_1, T_1) else 'does NOT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your own tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9283cb938111a6b393839cb3d3cc2bc",
     "grade": true,
     "grade_id": "check_tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = State(1, {'AP'})\n",
    "s2 = State(2)\n",
    "\n",
    "T0 = TransitionSystem({s1}, {s1}, set(), set())\n",
    "assert is_always_eventually_satisfied(1, T0) == True\n",
    "assert is_always_eventually_satisfied('AP', T0) == True\n",
    "\n",
    "T1 = TransitionSystem({s1}, {s1}, {'a'}, {Transition(s1, 'a', s1)})\n",
    "assert is_always_eventually_satisfied(1, T1) == True\n",
    "\n",
    "T2 = TransitionSystem({s1, s2}, {s1}, {'a'}, {Transition(s1, 'a', s2)})\n",
    "assert is_always_eventually_satisfied(1, T2) == False\n",
    "assert is_always_eventually_satisfied(2, T2) == True\n",
    "\n",
    "T3 = TransitionSystem({s1, s2}, {s1}, {'a'}, {Transition(s1, 'a', s1), Transition(s1, 'a', s2)})\n",
    "assert is_always_eventually_satisfied(1, T3) == False\n",
    "assert is_always_eventually_satisfied(2, T3) == True\n",
    "\n",
    "T4 = TransitionSystem({s1, s2}, {s1}, {'a'}, {Transition(s1, 'a', s1), \n",
    "                                              Transition(s1, 'a', s2), \n",
    "                                              Transition(s2, 'a', s2)})\n",
    "assert is_always_eventually_satisfied(1, T4) == False\n",
    "assert is_always_eventually_satisfied(2, T4) == True\n",
    "\n",
    "s3 = State(3)\n",
    "s4 = State(4, {3})\n",
    "T5 = TransitionSystem({s1, s2, s3, s4}, {s1, s2}, {'a', 'b'}, \n",
    "                      {Transition(s1, 'a', s3),\n",
    "                       Transition(s1, 'b', s4),\n",
    "                       Transition(s2, 'a', s4)})\n",
    "assert is_always_eventually_satisfied(\"AP\", T5) == False\n",
    "assert is_always_eventually_satisfied(2, T5) == False\n",
    "assert is_always_eventually_satisfied(3, T5) == True\n",
    "assert is_always_eventually_satisfied(4, T5) == False\n",
    "\n",
    "T6 = TransitionSystem({s1, s2, s3, s4}, {s1}, {'a', 'b'}, \n",
    "                      {Transition(s1, 'a', s2),\n",
    "                       Transition(s1, 'b', s3),\n",
    "                       Transition(s2, 'a', s1),\n",
    "                       Transition(s2, 'b', s4),\n",
    "                       Transition(s3, 'a', s1),\n",
    "                       Transition(s3, 'b', s4),\n",
    "                       Transition(s4, 'a', s2)})\n",
    "assert is_always_eventually_satisfied(1, T6) == True\n",
    "assert is_always_eventually_satisfied(2, T6) == True\n",
    "assert is_always_eventually_satisfied(3, T6) == True\n",
    "assert is_always_eventually_satisfied(4, T6) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ef0e33e01d6f744e47f4cfcec3bb8b",
     "grade": false,
     "grade_id": "cell-eb911511d14629a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fb4352a433820fb31b3fe7dec05190e",
     "grade": false,
     "grade_id": "cell-8ed6180918fb0dfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.4\n",
    "\n",
    "### Solving the Booking Problem *[1p]*\n",
    "\n",
    "It seems that there is an issue with the booking of the resources in the system (the Petri net from earlier) at some point. And that even pertains when we scale the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. multiplying all tokens by factor 7\n",
    "m_2 = '[21 0 14 0 14 7]'\n",
    "T_2 = make_transition_system(make_petrinet(p11_tokens=21, p21_tokens=14, R1_tokens=14, R2_tokens=7))\n",
    "print('T_2 {} satisfy phi_2!'.format('does' if is_always_eventually_satisfied(m_2, T_2) else 'does NOT'))\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE LINE BELOW BEFORE SUBMISSION !!!\n",
    "\"\"\"\n",
    "plot_transitionsystem(T_2, 'fig/T_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea7846c7117d82e067edf1e8a5a00220",
     "grade": false,
     "grade_id": "cell-81c92aab8a1c7cf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Given that observation:\n",
    "\n",
    "* Change $P_1$ so that the booking problem is resolved. Elements of the original structure of the Petri net (places and arcs) must not be removed in that. ***[1p]***\n",
    "* *Hint:* Based on the model you could figure out what the problem is, and start to experiment with the model to find a way to prevent the problem from occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5055c76d0df8ba5fc43fd4198f9fe358",
     "grade": false,
     "grade_id": "fix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import array_str\n",
    "\n",
    "\n",
    "def make_fixed_petrinet():\n",
    "    \"\"\"Makes a new, altered, issue-free version of P_1\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return P\n",
    "\n",
    "\n",
    "P_3 = make_fixed_petrinet()\n",
    "T_3 = make_transition_system(P_3)\n",
    "print('T_3 {} satisfy phi_3!'.format('does' if is_always_eventually_satisfied(array_str(P_3.init_marking), T_3)\n",
    "                                     else 'does NOT'))\n",
    "print('P_3:')\n",
    "plot_petrinet(P_3, 'fig/P_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f9e8e59a7b7a4c89c48e338f9b14818",
     "grade": false,
     "grade_id": "cell-394dfb346b1edbd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, you found a configuration of your model for which everything is fine. You get super excited and burst into your boss' office, and exclaim: _\"I know how to fix our problem with the system! Give me a raise!\"_ And your boss gets super excited and asks you how to solves her greatest nightmare. Obviously, you cannot say now: _\"We need to get a big can of paint and draw some circles and arrows on the floor!\"_ (except maybe if you know some ancient magic ritual against booking problems). No, you need to translate your model changes into your boss' world, i.e. do we need to change the process, buy new equipment, re-program the controllers, etc. Also, you cannot decide that one of the resources is no longer needed for processing or remove one of the two processes altogether. In other words, the places, transitions and arcs of the original petri net $P_1$ must remain. You can add places and so on, or adjust the number of tokens as long  as the original structure is part of the new model and you can motivate your changes in words that are understandable to your manager, who knows nothing about discrete event systems.\n",
    "\n",
    "* Describe briefly in words how you could implement your proposed changes in the real system. ***[1p]***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c767c7ccb37598cd4f0e949a517b137f",
     "grade": true,
     "grade_id": "implementation",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b50e4d222d7c457b732eec2e2deac242",
     "grade": false,
     "grade_id": "cell-04fd41381b92af64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Bonus Task (optional)\n",
    "\n",
    "* Can you derive a general rule to avoid the booking problem for the given Petri net structure of $P_1$, that is a booking problem with two parallel processes? If yes, explain it with a few words. ***[+1p]***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f520f0417d6081a6d1b0e2148040e25",
     "grade": true,
     "grade_id": "bonus",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e018bb429e8652ff4f741fc2a0f7d64",
     "grade": false,
     "grade_id": "cell-3aebb8bbe5d39250",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "357c9e99c2bd2e951ab8d38105c60e65",
     "grade": false,
     "grade_id": "cell-141c000a5ea6f1d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Reinforcement Learning\n",
    "\n",
    "After having had a tremendously successful day at work, you call your friends to share the story of how you single-handedly solved your company's booking issue using $\\mu$-calculus. All agree that this needs to be celebrated accordingly, and so you meet up at Andra Långgatan. Your likely future promotion is cause for a long and jolly evening, and when you finally decide to go home, it has become quite dark already. To make matters even worse, a strong wind is blowing from south/south-west, and you seem to have forgotten the way to your home in Gamlestaden (for whatever reason - blame it on the long day thinking about temporal logic if you'd like). You conclude that your best option would be to start walking in some direction in the hope of finding your home eventually. But careful! If you walk too close alongside the Göta Älv, the strong wind might blow you into the river. In addition to the unpleasant experience, the river will take you back to Järntorget and you need to start your journey home all over again. And if that wasn't bad enough yet, you might get convinced to join some _\"late-night studying\"_ if you pass by J. A. Pripps at Chalmers.   \n",
    "\n",
    "In this part of the assignment, you will implement a Reinforcement Learning algorithm called Q-learning. Reinforcement Learning uses data sampled from the plant (or the environment in RL terms) to derive an optimal controller - just right for finding your way back home.\n",
    "\n",
    "Let us look at the environment first:\n",
    "\n",
    "![WindyGothenburg](fig/windy_gothenburg.png)\n",
    "\n",
    "The available actions in this environment are: {'north', 'east', 'south', 'west'}. In addition to the action you take, the wind will blow you with a probability of 5% to the east and with a probability of 10% to the north. To start with, we simplify the environment by removing the river and J.A. Pripps. These two features will be added again later on. You can find the full code of the environment in `util/datastructures.py`. Here, we will import it and configure it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa2682e7ed386af60891e5a2ffe8e28",
     "grade": false,
     "grade_id": "cell-6dfafa26e37f7224",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from util import WindyGothenburg\n",
    "\n",
    "BASIC_ENV = {'name': 'Basic Env', \n",
    "             'w': 12, 'h': 12,                    # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},  # Coordinates of obstacles on the grid \n",
    "             'water': set(),                      # Coordinates of the Göta Älv (not included in this env)\n",
    "             'pripps_reward': None}               # Reward for passing by J.A. Pripps (not included in this env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39d94ae988c449c2460e80a21326e41f",
     "grade": false,
     "grade_id": "cell-7b0a3e55845aa420",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The RL algorithm that you will implement is as follows:\n",
    "\n",
    "**Algorithm 1.** Q-learning$(\\alpha, \\epsilon, \\gamma)$\n",
    "\n",
    ">Initialize $Q(x,a)$ arbitrarily\n",
    ">\n",
    ">**for all** episodes **do**\n",
    ">\n",
    ">>Initialize $x$\n",
    ">>\n",
    ">>**for all** steps of episode  **do**\n",
    ">>>\n",
    ">>>Choose $a$ in $x$ using policy derived from $Q$ (e.g. $\\epsilon$-greedy)\n",
    ">>>\n",
    ">>>Take action $u$, observe $r$, $x'$\n",
    ">>>\n",
    ">>>$Q(x,a) = Q(x,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(x', a') - Q(x,a) \\right]$\n",
    ">>>\n",
    ">>>$x = x'$\n",
    ">>>\n",
    ">>**end for**\n",
    ">\n",
    ">**end for**\n",
    ">\n",
    "**return** $\\pi(a) = argmax_a Q(x, a)$\n",
    "\n",
    "We have already implemented parts of this algorithm for you. Make sure to read carefully all the functions below. One of those functions creates the Q-table by using nested Python [dictionaries](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bfb7bde012adc3a6381d8afd065c29a",
     "grade": false,
     "grade_id": "cell-2939871e3c171dbc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def initialize_Q(states, actions, scaling_factor=0.1):\n",
    "    \"\"\"\n",
    "    Initializes the Q-table as a dictionary of dictionaries.\n",
    "    \n",
    "    A particular Q-value can be retrieved by calling Q[x][a].\n",
    "    All actions and their associated values in a state x can \n",
    "    be retrieved through Q[x].\n",
    "    Q-values are initialized to a small random value to encourage \n",
    "    exploration and to facilitate learning.\n",
    "    \n",
    "    :param states: iterable set of states\n",
    "    :param actions: iterable set of actions\n",
    "    \"\"\"\n",
    "    return {x: {a: random.random() * scaling_factor for a in actions} for x in states}\n",
    "\n",
    "def argmax_Q(Q, state):\n",
    "    \"\"\"Computes the argmax of Q in a particular state.\"\"\"\n",
    "    max_q = float(\"-inf\")\n",
    "    argmax_q = []\n",
    "    for a, q in Q[state].items():\n",
    "        if q == max_q:\n",
    "            argmax_q.append(a)\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            argmax_q = [a]\n",
    "    return random.choice(argmax_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c4c6f332d2055358790adc6a1f2439",
     "grade": false,
     "grade_id": "cell-05ba1591136d06b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44cade45720aa64a6482f2f77e2925c2",
     "grade": false,
     "grade_id": "cell-cb04ccc104be9b68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 2.1\n",
    "\n",
    "### Finding Home through Q-learning *[1p]*\n",
    "\n",
    "As a first task, implement a function that chooses with probability $1-\\epsilon$ the action with the highest $Q$-value in a given state (i.e. it chooses greedily), and with probability $\\epsilon$ a random action, where $0 < \\epsilon < 1$. This is a popular exploration strategy in RL that ensures that all states are visited theoretically infinitively often.\n",
    "\n",
    "* Implement the $\\epsilon$-greedy choice in code. \n",
    "* *Hint*: You might want to use the Python function [random.random()](https://docs.python.org/3/library/random.html#https://docs.python.org/3.7/library/random.html#random.random) and [random.choice()](https://docs.python.org/3/library/random.html#random.choice).\n",
    "* *Hint*: You might want to read the documentation of [dictionaries](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b9e8dca67734386b89411013321337b",
     "grade": false,
     "grade_id": "epsilon_greedy",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def choose_epsilon_greedily(Q, x, epsilon):\n",
    "    \"\"\"\n",
    "    Chooses random action with probability epsilon, else argmax_a(Q(*|x))\n",
    "    \n",
    "    :param Q: Q-table as dict of dicts\n",
    "    :param x: state\n",
    "    :param epsilon: float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c8692c6043aa1eea122293cd4de9fd8",
     "grade": false,
     "grade_id": "cell-15e9519922bab68d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Next, we need to implement a function that decides how fast our RL algorithm will be learning. Generally, the learning rate $\\alpha_k$ must satisfy the conditions $\\sum_{k=0}^\\infty \\alpha_k^2 < \\infty$ and $\\sum_{k=0}^\\infty \\alpha_k = \\infty$, to be able to guarantee that the estimates of Q converge to the optimal Q-function.\n",
    "\n",
    "* Implement a function that computes $\\alpha$ given the state-action visitation count $k$. \n",
    "* *Hint*: Check the lecture notes for further information.\n",
    "\n",
    "A correct implementation of both functions is needed to complete Task 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbf308a8fd8df8db8391c965960b15d2",
     "grade": false,
     "grade_id": "alpha",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_alpha(x, a, stateaction_visitation_counts, A, B):\n",
    "    \"\"\"\n",
    "    Returns a value of the learning rate.\n",
    "    \n",
    "    A particular state-action visitation count can be \n",
    "    retrieved by calling stateaction_visitation_count[x][a].\n",
    "    :param x: state\n",
    "    :param a: action\n",
    "    :param stateaction_visitation_counts: dictionary of dictonaries\n",
    "    :param A: integer parameter of the learning rate\n",
    "    :param B: integer parameter of the learning rate\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38981f9b3430ff6141240c95c0a663fd",
     "grade": false,
     "grade_id": "helper_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# epsilon-greedy tests\n",
    "Q1 = initialize_Q(states={1}, actions={'a'})\n",
    "assert choose_epsilon_greedily(Q1, 1, 0.1) == 'a'\n",
    "\n",
    "Q2 = initialize_Q(states={1}, actions={'a', 'b'})\n",
    "Q2[1]['a'] = 1\n",
    "Q2[1]['b'] = 0\n",
    "assert choose_epsilon_greedily(Q2, 1, 0.0) == 'a'\n",
    "\n",
    "epsilon = 0.1\n",
    "k = 0\n",
    "l = 0\n",
    "for m in range(1000):\n",
    "    action = choose_epsilon_greedily(Q2, 1, epsilon)\n",
    "    k = k + 1 if action == 'a' else k\n",
    "    l = l + 1 if action == 'b' else l\n",
    "assert k/m >= (1-epsilon)\n",
    "assert l/m > 0.0\n",
    "\n",
    "# learning rate tests\n",
    "x = (0, 0)\n",
    "svc = {x: {'a': 0}}\n",
    "assert 0.0 < get_alpha(x, 'a', svc, 1, 2) < 1.0\n",
    "assert 0.0 < get_alpha(x, 'a', svc, 1e10, 2e10) < 1.0\n",
    "svc = {x: {'a': 1e10}}\n",
    "assert 0.0 < get_alpha(x, 'a', svc, 1, 2) < 1.0\n",
    "assert 0.0 < get_alpha(x, 'a', svc, 1e10, 2e10) < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f734d3c78f63d109912010158cf7037",
     "grade": false,
     "grade_id": "cell-0860dfc12d3aa182",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, we can turn to the $Q$-learning algorithm. We have made a start with it already. \n",
    "\n",
    "* Now, implement the $Q$-value update from Algorithm 1. The relevant line is marked with a comment in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a93dd12f3f7fe0f93e5d195d02ff29a6",
     "grade": false,
     "grade_id": "learn_q_def",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def learn_q(env, epsilon, gamma, A, B, num_episodes=250, max_steps=100, render=False, test=False):\n",
    "    \n",
    "    Q = initialize_Q(env.states, env.actions, scaling_factor=0.1)\n",
    "    counts =  {x: {a: 0 for a in env.actions} for x in env.states}\n",
    "    \n",
    "    stats = {'avg_r_smoothed': 0, 'eps_goal_found': num_episodes, \n",
    "             'eps_goal_learned': num_episodes, 'max_r_smoothed': 0}\n",
    "    \n",
    "    if A>B:\n",
    "        if not test:\n",
    "            s = 'A cannot be greater than B. They are A = {} and B = {}.'.format(A, B) \n",
    "            print(s + ' Returning random policy and default learning statistics now.')\n",
    "        env.close()\n",
    "        stats = {'avg_r_smoothed': -np.inf, 'eps_goal_found': np.inf, \n",
    "             'eps_goal_learned': np.inf, 'max_r_smoothed': -np.inf}\n",
    "        return {x: argmax_Q(Q, x) for x in env.states}, stats\n",
    "    \n",
    "    for l in range(num_episodes):\n",
    "        # Reset for episode\n",
    "        x = env.reset()\n",
    "        done = False\n",
    "        sum_of_r = 0\n",
    "\n",
    "        for m in range(max_steps):\n",
    "            # Pick action\n",
    "            a = choose_epsilon_greedily(Q, x, epsilon)\n",
    "            next_x, r, done = env.step(a)  \n",
    "            \n",
    "            alpha = get_alpha(x, a, counts, A, B)\n",
    "                \n",
    "            # Update Q-Table\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Increment\n",
    "            x = next_x\n",
    "            sum_of_r += r\n",
    "            \n",
    "            if render:\n",
    "                env.render(Q)\n",
    "        \n",
    "            if done:\n",
    "                # Set the Q-values of the terminal state to 0\n",
    "                for action in Q[next_x].keys():\n",
    "                    Q[next_x][action] = 0\n",
    "                break\n",
    "        \n",
    "        # Track some statistics\n",
    "        avg_r = sum_of_r / (m+1)\n",
    "        stats['avg_r_smoothed'] = 0.95 * stats['avg_r_smoothed'] + 0.05 * avg_r\n",
    "        \n",
    "        if r == 100 and stats['eps_goal_found'] == num_episodes:\n",
    "            stats['eps_goal_found'] = l\n",
    "        if stats['avg_r_smoothed'] > 2.0 and stats['eps_goal_learned'] == num_episodes:\n",
    "            stats['eps_goal_learned'] = l\n",
    "        if stats['avg_r_smoothed'] > stats['max_r_smoothed']:\n",
    "            stats['max_r_smoothed'] = stats['avg_r_smoothed']\n",
    "\n",
    "        # Update plots\n",
    "        if not test:\n",
    "            env.render(Q, avg_r, stats['avg_r_smoothed'], l)\n",
    "    \n",
    "    env.close()\n",
    "    return {x: argmax_Q(Q, x) for x in env.states}, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04567fb9a5df54fb047e9ccf90dad9cc",
     "grade": false,
     "grade_id": "cell-caf9570be22c6c44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your task is now to:\n",
    "* play around with the Q-learning for the Basic WindyGothenburg environment and its parameters. In particular:\n",
    "  * Choose a value for $\\epsilon$\n",
    "  * Choose a value for the discount factor $\\gamma$\n",
    "  * Choose a value for the count-based learning parameters $A$ and $B$\n",
    "* Your Q-learning will return the learned policy, which is evaluated in the test cell below. \n",
    "* If your parameters produces policies that get you home in more than 75% of the learning repetitions, we are satisfied. ***[1p]*** \n",
    "* _Hint_: Make sure to truly understand our evaluation criteria.\n",
    "* _Hint_: Each parameter affects the learning differently. What values for each parameter would likely produce the wanted behavior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "902e6d24d743b8758b2b1b960e275294",
     "grade": false,
     "grade_id": "cell-fd1bedcc8b202c2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If you would like to watch the episode, set render = True\n",
    "# For updates only at the end of each episode, set render = False\n",
    "# render = False is significantly faster.\n",
    "render = False\n",
    "\n",
    "# Choose values for epsilon, gamma, A, and B\n",
    "epsilon = None\n",
    "gamma = None\n",
    "A = None\n",
    "B = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES BELOW BEFORE SUBMISSION !!!\n",
    "\"\"\"\n",
    "\n",
    "env = WindyGothenburg(**BASIC_ENV)\n",
    "control_policy = learn_q(env, epsilon, gamma, A=A, B=B, num_episodes=250, render=render)\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES ABOVE BEFORE SUBMISSION !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a757128586d9bf89ed0546a14f136d3",
     "grade": false,
     "grade_id": "eval_param",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "metrics = ['avg_success_rate', 'avg_eps_goal_found', 'avg_eps_goal_learned', 'avg_max_r_smoothed']\n",
    "\n",
    "def eval_learning_params(EnvClass, env_config, learning_func, epsilon, gamma, A, B, pripps_reward=None, \n",
    "                         num_episodes=250, repeats=250, max_steps_for_success=20):\n",
    "    \"\"\"\n",
    "    Evaluates the hyperparameters of a learning function \n",
    "    by repeating the learning several times. Each time, it \n",
    "    is checked whether the learned policy is close to the \n",
    "    true optimal policy.\n",
    "    \"\"\"\n",
    "    averages = {'avg_success_rate': 0, 'avg_eps_goal_found': 0, \n",
    "                  'avg_eps_goal_learned': 0, 'avg_max_r_smoothed': 0}\n",
    "    \n",
    "    for _ in range(repeats):\n",
    "        # Learn policy\n",
    "        env = EnvClass(**env_config | {'pripps_reward': pripps_reward}, test=True)\n",
    "        control_policy, stats = learning_func(env, epsilon=epsilon, gamma=gamma, A=A, B=B, num_episodes=num_episodes,\n",
    "                                 max_steps=100, render=False, test=True)\n",
    "        x = env.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "        # Evaluate learned policy\n",
    "        while not done and i < max_steps_for_success:\n",
    "            x, r, done = env.step(control_policy.get(x))\n",
    "            i += 1\n",
    "        averages['avg_success_rate'] += 1 if r == 100 else 0\n",
    "        averages['avg_eps_goal_found'] += stats['eps_goal_found']\n",
    "        averages['avg_eps_goal_learned'] += stats['eps_goal_learned']\n",
    "        averages['avg_max_r_smoothed'] += stats['max_r_smoothed']\n",
    "        \n",
    "    return {metric: value/repeats for metric, value in averages.items()} | {'epsilon': epsilon, \n",
    "                                                                            'gamma': gamma, 'A': A, 'B': B,\n",
    "                                                                            'pripps_reward': pripps_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02b635fd8e3ce031ab2e0900f9c31fa0",
     "grade": true,
     "grade_id": "success_075",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "BASIC_ENV = {'name': 'Basic Env', \n",
    "             'w': 12, 'h': 12,                    # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},  # Coordinates of obstacles on the grid \n",
    "             'water': set(),                      # Coordinates of the Göta Älv (not included in this env)\n",
    "             'pripps_reward': None}               # Reward for passing by J.A. Pripps (not included in this env)\n",
    "\n",
    "stats = eval_learning_params(WindyGothenburg, BASIC_ENV, learn_q, \n",
    "                             epsilon=epsilon, gamma=gamma, A=A, B=B, \n",
    "                             num_episodes=250, repeats=250, max_steps_for_success=16)\n",
    "\n",
    "assert stats['avg_success_rate'] > 0.75, 'Got {} instead'.format(stats['avg_success_rate'])\n",
    "print(\"The achieved average success rate was \", stats['avg_success_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f9bc6083afe4492a4070e0597786894",
     "grade": false,
     "grade_id": "cell-01abc3c9adfaffd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38db040ad6473e128033d17471b9ade1",
     "grade": false,
     "grade_id": "cell-aa0bdaaf74b6e4e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 2.2\n",
    "\n",
    "### Reflections on Hyperparameters of the Basic Environment *[1p]*\n",
    "\n",
    "You may have noticed by now that different $\\epsilon$ or $\\gamma$ lead to different learning behaviors. We have implemented a function for you that allows for plotting a grid of hyperparameters. This will hopefully give you better insight into tuning the hyperparameters of the reinforcement learning algorithm. Beside the average success rate (see code above), the function plots three more statistics that are the averages of these values:\n",
    "\n",
    "![statistics](fig/learning_statistics.png)\n",
    "\n",
    "Although we choose to prioritize the success rate, a high average maximum reward is often the key metric reported on in many research papers. \n",
    "\n",
    "The code below executes the learning in parallel on all available CPU-cores of your machine and plots the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cefdabfb032d2fdb571035bc31a0ab56",
     "grade": false,
     "grade_id": "cell-32a7d698848e259d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "from collections import namedtuple\n",
    "from util import validate_grid_config, write_defs_to_file, plot_heatmaps\n",
    "\n",
    "HyperParameterGrid = namedtuple('HyperParameterGrid', ['epsilon', 'gamma', 'A', 'B', 'pripps_reward'], \n",
    "                                defaults=([0], [0], [0], [0], [None]))\n",
    "\n",
    "def eval_hyperparam_grid(env_config, param_grid, num_episodes=250, repeats=250, max_steps_for_success=20, \n",
    "                         log_y=False, save_name=None):\n",
    "    \"\"\"\n",
    "    Executes the function `eval_learning_params` concurrently for \n",
    "    all combinations of the parameters in param_grid.\n",
    "    \"\"\"\n",
    "    X, Y, x_name, y_name, title = validate_grid_config(env_config, param_grid)                \n",
    "    write_defs_to_file(initialize_Q, argmax_Q, choose_epsilon_greedily, get_alpha, \n",
    "                       learn_q, eval_learning_params, f'./tmp.py')    # Prepare for multiprocessing    \n",
    "    from tmp import task, learning_f\n",
    "    print('Evaluating {} hyperparameter combinations ...'.format(len(X)*len(Y)))\n",
    "    with Pool() as p:\n",
    "        stats = p.starmap(task, product([WindyGothenburg], [env_config], [learning_f], \n",
    "                                        param_grid.epsilon, param_grid.gamma, \n",
    "                                        param_grid.A, param_grid.B, param_grid.pripps_reward,\n",
    "                                        [num_episodes], [repeats], [max_steps_for_success]))\n",
    "    fig = plot_heatmaps(stats, X, Y, metrics, x_name, y_name, save_name, title, log_y=log_y)\n",
    "    os.remove(f'./tmp.py')\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e38da8b528534a82909bbe4eaf886269",
     "grade": false,
     "grade_id": "cell-5a2ae452bfb5dfc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By that we can evaluate a hyperparameter grid like this: choose two hyperparameters that you would like to evaluate, define their levels as elements of a list (the combination of these become the data points of the grid), pass in the values of the other hyperparameters as lists with a single element. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpg = HyperParameterGrid(epsilon=[0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99], \n",
    "                         gamma=[0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99], \n",
    "                         A=[50], B=[100])\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES BELOW BEFORE SUBMISSION !!!\n",
    "\"\"\"\n",
    "\n",
    "stats = eval_hyperparam_grid(BASIC_ENV, hpg, max_steps_for_success=16, \n",
    "                             save_name='hpg_basic_eps_gamma')\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES ABOVE BEFORE SUBMISSION !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3210d01ebc60bc10323f9e8909ca1bce",
     "grade": false,
     "grade_id": "cell-bf149830114caf1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now,\n",
    "* Reflect on the difference in hyperparameters that lead to a high average success rate (plot on the upper left) versus a high average maximum reward (plot on the upper right).\n",
    "* How is the optimal policy related to these two statistics?\n",
    "* Which of the two statistics is more relevant? And which hyperparameters would you choose? \n",
    "* Write down your insights in a few brief sentences. ***[1p]***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18e6cacec4cd28a12f4bd23eb584ed01",
     "grade": true,
     "grade_id": "success_metrics",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08e1e20a11d3a49cc2553944021d6359",
     "grade": false,
     "grade_id": "cell-f61c81af7a807fb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf24c4703bfb1563cd23df823139416",
     "grade": false,
     "grade_id": "cell-9a248cdcf17a75eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2.3\n",
    "\n",
    "### Find Hyperparameters of the River Environment that Lead to > 94% Success *[1p]*\n",
    "\n",
    "Now, we add back the Göta Älv to the environment. This will change a few things for the learning.\n",
    "* Tune the RL algorithm again such that a success rate of > 94 % is achieved. ***[1p]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e7cb5eb217ff940f3744d004b060b7",
     "grade": false,
     "grade_id": "cell-b12cac2de52c8ccd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "RIVER_ENV = {'name': 'River Env',\n",
    "             'w': 12, 'h': 12,                            # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},          # Coordinates of obstacles on the grid \n",
    "             'water': {(k, 12) for k in range(2, 12)},    # Coordinates of the Göta Älv \n",
    "             'pripps_reward': None}                       # Reward for passing by J.A. Pripps (not included in this env)\n",
    "\n",
    "river_epsilon = None\n",
    "river_gamma = None\n",
    "river_A = None\n",
    "river_B = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your own experiments and tests\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT ALL CALLS TO eval_hyperparam_grid BEFORE SUBMISSION !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4584f47dfb05d39af18cb3191bb25774",
     "grade": true,
     "grade_id": "river_success",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "RIVER_ENV = {'name': 'River Env',\n",
    "             'w': 12, 'h': 12,                            # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},          # Coordinates of obstacles on the grid \n",
    "             'water': {(k, 12) for k in range(2, 12)},    # Coordinates of the Göta Älv \n",
    "             'pripps_reward': None}                       # Reward for passing by J.A. Pripps (not included in this env)\n",
    "\n",
    "stats = eval_learning_params(WindyGothenburg, RIVER_ENV, learn_q, \n",
    "                             epsilon=river_epsilon, gamma=river_gamma, \n",
    "                             A=river_A, B=river_B, \n",
    "                             max_steps_for_success=20, repeats=1000)\n",
    "assert stats['avg_success_rate'] > 0.94, 'Got {} instead'.format(stats['avg_success_rate'])\n",
    "print(\"The achieved average success rate was \", stats['avg_success_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88cedf69547b48dde53e379a18bce22b",
     "grade": false,
     "grade_id": "cell-054e1636de18935e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2c82e0bedea193ca312da55725c1c1c",
     "grade": false,
     "grade_id": "cell-5d71ddfa490dff6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2.4\n",
    "\n",
    "### Reflect on the Impact of Intermediate Rewards *[1p]*\n",
    "\n",
    "As the last part of the task, you will need to decide how much a visit to J.A. Pripps is worth to you. Note that this is strictly speaking not an element of the Q-learning algorithm, but rather an element of the reward function. While many publications on RL assume that the reward function is given, in practice, designing a good reward function, that enables the RL agent to learn the wanted behavior, is a difficult task. The complications often arise from balancing intermediate rewards, that can give continuous learning feedback, and terminal rewards, that indicate goal fulfillment. `pripps_reward` is an intermediate reward whereas the terminal rewards of `+100` for getting home or `-10` for falling into the river are terminal rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3212df35a1a20981720c0dfe38e98927",
     "grade": false,
     "grade_id": "pripps",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PRIPPS_ENV = {'name': 'Pripps Env',\n",
    "             'w': 12, 'h': 12,                            # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},          # Coordinates of obstacles on the grid \n",
    "             'water': {(k, 12) for k in range(2, 12)},    # Coordinates of the Göta Älv \n",
    "             'pripps_reward': 1.828}                      # Reward for passing by J.A. Pripps \n",
    "\n",
    "pripps_epsilon = None\n",
    "pripps_gamma = None\n",
    "pripps_A = None\n",
    "pripps_B = None\n",
    "pripps_reward = 1.828\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your own experiments and tests\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT ALL CALLS TO eval_hyperparam_grid BEFORE SUBMISSION !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d085d769ede1405d2f853179c1a315bd",
     "grade": false,
     "grade_id": "pripps_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PRIPPS_ENV = {'name': 'Pripps Env',\n",
    "             'w': 12, 'h': 12,                            # width and height of the grid\n",
    "             'obstacles': {(2,9), (3,9), (4,9),\n",
    "                          (8,9), (9,9), (10,9)},          # Coordinates of obstacles on the grid \n",
    "             'water': {(k, 12) for k in range(2, 12)},    # Coordinates of the Göta Älv \n",
    "             'pripps_reward': 1.828}                      # Reward for passing by J.A. Pripps \n",
    "\n",
    "stats = eval_learning_params(WindyGothenburg, PRIPPS_ENV, learn_q, \n",
    "                             epsilon=pripps_epsilon, gamma=pripps_gamma, \n",
    "                             A=pripps_A, B=pripps_B, pripps_reward=pripps_reward,\n",
    "                             max_steps_for_success=20)\n",
    "assert stats['avg_success_rate'] > 0.9, 'Got {} instead'.format(stats['avg_success_rate'])\n",
    "print(\"The achieved average success rate was \", stats['avg_success_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ec37df017ac9b17362ba492305751ed",
     "grade": false,
     "grade_id": "cell-1b365d8cbf543f04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* Reflect on this part of the assignment and write down your insights in a few brief sentences. ***[1p]***\n",
    "* Make sure to touch upon:\n",
    " * the final value of `pripps_reward` that you chose and why you have chosen that value;\n",
    " * the impact of `pripps_reward` and $\\gamma$ on solving the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31ffd2d7ccc589f3303950a7da251fc9",
     "grade": true,
     "grade_id": "pripps_reflections",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32f509d0422c74402a5de11b88aa801e",
     "grade": false,
     "grade_id": "cell-e53c87aa4cdb8edf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdf45a1d97172bc8702a1c32c0d4b1f6",
     "grade": false,
     "grade_id": "cell-ca4413be58012d08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "That is all there is. If you are done,\n",
    "\n",
    "* Save the notebook\n",
    "* Upload the .ipynb file to Canvas\n",
    "* Tell your teammate how much you appreciated their invaluable insights and how fun it was to collaborate with them on the assignments."
   ]
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
